{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAML pretrain  \n",
    "num_epochs=100<br>   \n",
    "inner_lr: 0.01 / 0.005 / 0.001 / 0.0005 <br>\n",
    "meta_lr :  0.0005 / 0.0004 / 0.0003 / 0.0003 / 0.0001 <br>\n",
    "meta_loss_sum / K : True, False <br>\n",
    "valid loader use : True, False <br>\n",
    "Activation function : [relu, sigmoid] <br>\n",
    "\n",
    "\n",
    "#### Pretrain\n",
    "epochs=200,<br> lr=0.0001<br> <br>\n",
    "Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "# from src.tasks import Sine_Task, Sine_Task_Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "from models.utils import *\n",
    "import os\n",
    "import datetime\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from lifelines.utils import concordance_index\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchsummary import summary as summary\n",
    "# from models.tasks import Workload_Task\n",
    "# from pytorchtools import EarlyStopping\n",
    "\n",
    "# interpreter : py3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "data load and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "# torch.cuda.manual_seed(0)\n",
    "\n",
    "sample_path = 'data/rocksdb_benchmark_'\n",
    "batch_size = 32\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "group_dim = 32\n",
    "wk_vec_dim = 4\n",
    "metrics = ['TIME', 'RATE', 'WAF', 'SA']\n",
    "wk_vec = [[9,1,0,1], [9,1,0,4], [9,1,0,16], [9,1,0,64],\n",
    "            [1,1,0,1], [1,1,0,4], [1,1,0,16], [1,1,0,64],\n",
    "            [1,9,0,1], [1,9,0,4], [1,9,0,16], [1,9,0,64],\n",
    "            [0,0,1,1], [0,0,1,4], [0,0,1,16], [0,0,1,64]]\n",
    "\n",
    "wk_using_training_step = [0,1,3,\n",
    "                          4,5,7,\n",
    "                          8,9,11,\n",
    "                          12,13,15]\n",
    "K_tr = 32   # batch size of training step\n",
    "# K_te = 10   # batch size of testing step\n",
    "# wk_using_testing_step = [2,6,10,14]\n",
    "\n",
    "def raw_data_make(metrics, wk_vec, sample_path='data/rocksdb_benchmark_'):\n",
    "    entire_X_tr = pd.DataFrame()\n",
    "    entire_X_te = pd.DataFrame()\n",
    "    entire_y_tr = pd.DataFrame()\n",
    "    entire_y_te = pd.DataFrame()\n",
    "\n",
    "    for wk in range(16):\n",
    "        raw_data = pd.read_csv(sample_path+f'{wk}.csv')\n",
    "        y = raw_data[['RATE']]\n",
    "        x = raw_data.drop(columns=metrics)\n",
    "        pd_wk_info = pd.DataFrame(data=np.repeat([wk_vec[wk]], x.shape[0], axis=0), columns=[\"READ\", \"WRITE\", \"UPDATE\", \"VALUE_SIZE\"])\n",
    "        X = pd.concat((x, pd_wk_info), axis=1)\n",
    "\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1004)\n",
    "        entire_X_tr = pd.concat((entire_X_tr, X_tr))\n",
    "        entire_X_te = pd.concat((entire_X_te, X_te))\n",
    "        entire_y_tr = pd.concat((entire_y_tr, y_tr))\n",
    "        entire_y_te = pd.concat((entire_y_te, y_te))\n",
    "\n",
    "    return entire_X_tr, entire_X_te, entire_y_tr, entire_y_te\n",
    "\n",
    "entire_X_tr, entire_X_te, entire_y_tr, entire_y_te = raw_data_make(metrics, wk_vec, sample_path)\n",
    "\n",
    "scaler_X = MinMaxScaler().fit(entire_X_tr)\n",
    "\n",
    "#scaler_X = entire_X_tr\n",
    "scaler_y = StandardScaler().fit(entire_y_tr)\n",
    "\n",
    "input_dim = entire_X_tr.shape[-1]\n",
    "\n",
    "# K = 32  # batch size\n",
    "\n",
    "\n",
    "\n",
    "# WK_NUM = 16\n",
    "# EX_NUM=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make training step dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for MAML model dataloader in training step\n",
    "Train_task_MAML_DL_tr, Train_task_MAML_DL_te, Train_task_MAML_X_te, Train_task_MAML_y_te = MAML_dataset(entire_X_tr, entire_y_tr, entire_X_te, entire_y_te, scaler_X, scaler_y, wk_using_training_step, batch_size=K_tr)\n",
    "\n",
    "###################################################################\n",
    "Train_task_MAML_DL_val, _, _, _ = MAML_dataset(entire_X_tr, entire_y_tr, entire_X_te, entire_y_te, scaler_X, scaler_y, wk_using_training_step, batch_size=K_tr)\n",
    "###################################################################\n",
    "\n",
    "# for pretrain model dataloader in training step\n",
    "Train_task_pretrain_DL_tr, Train_task_pretrain_DL_te, Train_task_pretrain_X_te, Train_task_pretrain_y_te = pretrain_dataset(entire_X_tr, entire_y_tr, entire_X_te, entire_y_te, scaler_X, scaler_y, wk_using_training_step, batch_size=K_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "To implement MAML in Pytorch, we will need a model that can be easily parametrised with a set of weights that's distinct from the model's own parameters. Having these weights distinct from the model.parameters() allows us to easily make differentiable gradient updates within the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ReshapeNet - activation : ReLU'''\n",
    "class ReshapeNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16, group_dim=32, wk_vec_dim=4, output_dim=1):\n",
    "        super(ReshapeNet, self).__init__()\n",
    "        self.input_dim = input_dim - wk_vec_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.group_dim = group_dim\n",
    "        self.wk_vec_dim = wk_vec_dim\n",
    "\n",
    "        self.embedding = nn.Linear(self.wk_vec_dim, self.hidden_dim)\n",
    "        #self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim*self.group_dim), nn.Sigmoid()) # (22, 1) -> (group*hidden, 1)\n",
    "        self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim*self.group_dim), nn.ReLU()) # (22, 1) -> (group*hidden, 1)\n",
    "        self.attention = nn.MultiheadAttention(self.hidden_dim, 1)\n",
    "        #self.active = nn.Sigmoid()\n",
    "        self.activate = nn.ReLU()\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        wk = x[:, -self.wk_vec_dim:] # only workload information\n",
    "        x = x[:, :-self.wk_vec_dim] # only knobs\n",
    "        \n",
    "        self.embed_wk = self.embedding(wk) # (batch, 4) -> (batch, dim)\n",
    "        self.embed_wk = self.embed_wk.unsqueeze(1) # (batch, 1, dim)\n",
    "        self.x = self.knob_fc(x) # (batch, 22) -> (batch, group*hidden)\n",
    "        self.res_x = torch.reshape(self.x, (-1, self.group_dim, self.hidden_dim)) # (batch, group, hidden)\n",
    "        \n",
    "        # attn_ouptut = (1, batch, hidden), attn_weights = (batch, 1, group)\n",
    "        self.attn_output, self.attn_weights = self.attention(self.embed_wk.permute((1,0,2)), self.res_x.permute((1,0,2)), self.res_x.permute((1,0,2)))\n",
    "        self.attn_output = self.activate(self.attn_output.squeeze())\n",
    "        outs = self.attn_output\n",
    "        self.outputs = self.fc(outs)  \n",
    "        return self.outputs\n",
    "    \n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        # https://easy-going-programming.tistory.com/11\n",
    "        wk = x[:, -self.wk_vec_dim:] # only workload information\n",
    "        x = x[:, :-self.wk_vec_dim] # only knobs\n",
    "        embed_wk = F.linear(wk, weights[0], weights[1])\n",
    "        embed_wk = embed_wk.unsqueeze(1)\n",
    "        x = F.linear(x, weights[2], weights[3] )    # (22, 1) -> (group*hidden, 1)\n",
    "        #x = F.sigmoid(x)\n",
    "        x = F.relu(x)\n",
    "        self.p_res_x = torch.reshape(x, (-1, self.group_dim, self.hidden_dim)) # (batch, group, hidden)\n",
    "        # attn_output, self.attn_weights = self.attention(embed_wk.permute((1,0,2)), res_x.permute((1,0,2)), res_x.permute((1,0,2)))\n",
    "        attn_output, _ = F.multi_head_attention_forward(embed_wk.permute((1,0,2)), self.p_res_x.permute((1,0,2)), self.p_res_x.permute((1,0,2)), self.hidden_dim, 1, \n",
    "                                                        weights[4], weights[5], \n",
    "                                                        None, None, False, 0,\n",
    "                                                        weights[6], weights[7]) # self.attention(embed_wk.permute((1,0,2)), res_x.permute((1,0,2)), res_x.permute((1,0,2)))\n",
    "        #attn_output = F.sigmoid(attn_output.squeeze())\n",
    "        attn_output = F.relu(attn_output.squeeze())\n",
    "        outputs = F.linear(attn_output, weights[8], weights[9])\n",
    "\n",
    "        return outputs                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''ReshapeNet - activation : Sigmoid'''\n",
    "# class ReshapeNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=16, group_dim=32, wk_vec_dim=4, output_dim=1):\n",
    "#         super(ReshapeNet, self).__init__()\n",
    "#         self.input_dim = input_dim - wk_vec_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.group_dim = group_dim\n",
    "#         self.wk_vec_dim = wk_vec_dim\n",
    "\n",
    "#         self.embedding = nn.Linear(self.wk_vec_dim, self.hidden_dim)\n",
    "#         #self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim*self.group_dim), nn.Sigmoid()) # (22, 1) -> (group*hidden, 1)\n",
    "#         self.knob_fc = nn.Sequential(nn.Linear(self.input_dim, self.hidden_dim*self.group_dim), nn.Sigmoid()) # (22, 1) -> (group*hidden, 1)\n",
    "#         self.attention = nn.MultiheadAttention(self.hidden_dim, 1)\n",
    "#         #self.active = nn.Sigmoid()\n",
    "#         self.activate = nn.Sigmoid()\n",
    "#         self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         wk = x[:, -self.wk_vec_dim:] # only workload information\n",
    "#         x = x[:, :-self.wk_vec_dim] # only knobs\n",
    "        \n",
    "#         self.embed_wk = self.embedding(wk) # (batch, 4) -> (batch, dim)\n",
    "#         self.embed_wk = self.embed_wk.unsqueeze(1) # (batch, 1, dim)\n",
    "#         self.x = self.knob_fc(x) # (batch, 22) -> (batch, group*hidden)\n",
    "#         self.res_x = torch.reshape(self.x, (-1, self.group_dim, self.hidden_dim)) # (batch, group, hidden)\n",
    "        \n",
    "#         # attn_ouptut = (1, batch, hidden), attn_weights = (batch, 1, group)\n",
    "#         self.attn_output, self.attn_weights = self.attention(self.embed_wk.permute((1,0,2)), self.res_x.permute((1,0,2)), self.res_x.permute((1,0,2)))\n",
    "#         self.attn_output = self.activate(self.attn_output.squeeze())\n",
    "#         outs = self.attn_output\n",
    "#         self.outputs = self.fc(outs)  \n",
    "#         return self.outputs\n",
    "    \n",
    "#     def parameterised(self, x, weights):\n",
    "#         # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "#         # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "#         # https://easy-going-programming.tistory.com/11\n",
    "#         wk = x[:, -self.wk_vec_dim:] # only workload information\n",
    "#         x = x[:, :-self.wk_vec_dim] # only knobs\n",
    "#         embed_wk = F.linear(wk, weights[0], weights[1])\n",
    "#         embed_wk = embed_wk.unsqueeze(1)\n",
    "#         x = F.linear(x, weights[2], weights[3] )    # (22, 1) -> (group*hidden, 1)\n",
    "#         #x = F.sigmoid(x)\n",
    "#         x = self.activate(x)\n",
    "#         self.p_res_x = torch.reshape(x, (-1, self.group_dim, self.hidden_dim)) # (batch, group, hidden)\n",
    "#         # attn_output, self.attn_weights = self.attention(embed_wk.permute((1,0,2)), res_x.permute((1,0,2)), res_x.permute((1,0,2)))\n",
    "#         attn_output, _ = F.multi_head_attention_forward(embed_wk.permute((1,0,2)), self.p_res_x.permute((1,0,2)), self.p_res_x.permute((1,0,2)), self.hidden_dim, 1, \n",
    "#                                                         weights[4], weights[5], \n",
    "#                                                         None, None, False, 0,\n",
    "#                                                         weights[6], weights[7]) # self.attention(embed_wk.permute((1,0,2)), res_x.permute((1,0,2)), res_x.permute((1,0,2)))\n",
    "#         #attn_output = F.sigmoid(attn_output.squeeze())\n",
    "#         attn_output = self.activate(attn_output.squeeze())\n",
    "#         outputs = F.linear(attn_output, weights[8], weights[9])\n",
    "\n",
    "#         return outputs                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step of MAML,  normal pretrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML_one_batch():\n",
    "    def __init__(self, model, train_dataloaders, val_dataloaders, test_dataloaders, num_epochs, inner_lr, meta_lr, inner_steps=1, meta_mean=False, dot=True, lamb=0.6):\n",
    "    # def __init__(self, model, train_dataloaders, test_dataloaders, meta_tasks, inner_lr, meta_lr, K=K, inner_steps=1):   # K : number of sample data of task\n",
    "        \n",
    "        #############################################\n",
    "        self.patience = 10  # for early stopping \n",
    "        #############################################\n",
    "        self.meta_mean = meta_mean\n",
    "\n",
    "        # important objects\n",
    "        self.model = model        \n",
    "        self.train_dataloaders = train_dataloaders\n",
    "        self.val_dataloaders = val_dataloaders\n",
    "        self.test_dataloaders = test_dataloaders\n",
    "\n",
    "        # hyperparameters\n",
    "        self.num_epochs = num_epochs\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.dot = dot\n",
    "        self.lamb = lamb\n",
    "\n",
    "        # self.meta_tasks = meta_tasks    # list of using workload number for MAML\n",
    "        self.num_meta_tasks = len(self.train_dataloaders)    # len(train_dataloaders) = len(test_dataloaders) \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.weights = list(self.model.parameters()) # the maml weights we will be meta-optimising\n",
    "        # ######################################################\n",
    "        # # self.weights = list(torch.nn.init.xavier_uniform_(self.model.parameters())) # the maml weights we will be meta-optimising\n",
    "        # self.weights = list(self.model.parameters()) # the maml weights we will be meta-optimising\n",
    "        # for i in range(len(self.weights)):\n",
    "        #     self.weights[i]=torch.nn.init.kaiming_normal_(self.weights[i])\n",
    "        # ######################################################\n",
    "        self.meta_optimizer = torch.optim.Adam(self.weights, self.meta_lr)\n",
    "        \n",
    "        # self.K = K\n",
    "        self.inner_steps = inner_steps # with the current design of MAML, >1 is unlikely to work well         \n",
    "        \n",
    "        # metrics\n",
    "        self.meta_losses_tr = []\n",
    "        self.meta_losses_te = []\n",
    "        self.r2_score = []\n",
    "        self.best_loss = np.inf\n",
    "        # self.best_loss = \n",
    "\n",
    "    \n",
    "    def inner_loop(self, iter):     # i: task , iteration : iteration\n",
    "        # reset inner model to current maml weights\n",
    "        temp_weights = [w.clone() for w in self.weights]         \n",
    "        # perform training on data sampled from task\n",
    "\n",
    "        X, y = self.sample_tr[0], self.sample_tr[1]\n",
    "        ##########################################################\n",
    "        X_val, y_val = self.sample_val[0], self.sample_val[1]\n",
    "        ###########################################################\n",
    "        inner_loss = self.criterion(self.model.parameterised(X, temp_weights), y)\n",
    "        grad = torch.autograd.grad(inner_loss, temp_weights)\n",
    "        temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "\n",
    "        temp_pred = self.model.parameterised(X_val, temp_weights)\n",
    "        # calculate loss for update maml weight (with update inner loop weight)\n",
    "        if self.dot:\n",
    "            d = torch.bmm(self.model.p_res_x, self.model.p_res_x.transpose(1, 2))\n",
    "            dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(X.shape[0], 1, 1).cuda())\n",
    "            meta_loss = (1-self.lamb)*self.criterion(temp_pred, y) + self.lamb*dot_loss\n",
    "            # meta_loss = (1-self.lamb)*F.mse_loss(self.model.parameterised(X, temp_weights), y) + self.lamb*dot_loss\n",
    "        else:\n",
    "            meta_loss = self.self.criterion(temp_pred, y_val)\n",
    "        \n",
    "        return inner_loss, meta_loss\n",
    "\n",
    "    def main_loop(self):\n",
    "        # epoch_loss = 0 ####\n",
    "        trigger_times = 0\n",
    "        breaker = False ####\n",
    "        for e in range(1, self.num_epochs+1):    # epoch\n",
    "            sampler_tr = Sampler(dataloaders=self.train_dataloaders)\n",
    "            #################################################################\n",
    "            sampler_val = Sampler(dataloaders=self.val_dataloaders)\n",
    "            #################################################################    \n",
    "            for iter in range(1, len(self.train_dataloaders[0])+1):    # iteration       \n",
    "                total_meta_loss_tr = 0\n",
    "   \n",
    "                self.meta_optimizer.zero_grad()\n",
    "                sample_tr = sampler_tr.get_sample()\n",
    "                sample_val = sampler_val.get_sample()\n",
    "                meta_loss_sum = 0\n",
    "                wk_loss =[]                     \n",
    "                for num_wk in range(self.num_meta_tasks):\n",
    "                    self.sample_tr = sample_tr[num_wk]\n",
    "                    self.sample_val = sample_val[num_wk]  ###                  \n",
    "                    _, meta_loss = self.inner_loop(iter)\n",
    "                    wk_loss.append(meta_loss)\n",
    "                    meta_loss_sum += meta_loss   # i: task                               \n",
    "                # print(f'meta_loss_sum : {meta_loss_sum}')   ####\n",
    "                if self.meta_mean ==True :\n",
    "                    meta_loss_sum /= self.num_meta_tasks    ####\n",
    "                # print(f'meta_loss_mean : {meta_loss_sum}')  ####\n",
    "                total_meta_loss_tr += meta_loss_sum.item()\n",
    "                meta_loss_tr = total_meta_loss_tr/len(self.train_dataloaders)\n",
    "                \n",
    "                # compute meta gradient of loss with respect to maml weights\n",
    "                meta_loss_sum.backward()\n",
    "                self.meta_optimizer.step()\n",
    "            loss_te, te_outputs, r2_res = self.validate(model)  ###\n",
    "\n",
    "            # self.meta_losses_tr +=[meta_loss_sum]\n",
    "            self.meta_losses_tr +=[meta_loss_tr]\n",
    "            self.meta_losses_te += [loss_te]       \n",
    "            self.r2_score += [r2_res]      \n",
    "\n",
    "            time_ = datetime.datetime.today().strftime('%y%m%d/%H:%M:%S')\n",
    "            print(f\"{time_}[{e:02d}/{self.num_epochs}] meta_loss: {meta_loss_sum:.4f} loss_te: {loss_te:.4f}, r2_res = {r2_res:.4f}\")\n",
    "\n",
    "            # print(f\"{time_}[{e:02d}/{self.num_epochs}] meta_loss: {meta_loss_sum:.4f} loss_te: {loss_te:.4f}, r2_res = {r2_res:.4f} / trrigger times : {trigger_times}\")\n",
    "\n",
    "            # if loss_te < self.best_loss:\n",
    "            #     self.best_loss = loss_te\n",
    "            #     self.best_ouputs = te_outputs\n",
    "            #     self.best_r2_score = r2_res\n",
    "            #     self.best_model = self.model\n",
    "            #     self.best_epoch_num = e\n",
    "\n",
    "            ###############################################################\n",
    "            # early stopping\n",
    "            if loss_te > self.best_loss:\n",
    "                trigger_times += 1\n",
    "                # print('Trigger Times:' , trigger_times)\n",
    "                if trigger_times >= self.patience:\n",
    "                    print('Early stopping! \\n training step finish')\n",
    "                    breaker = True\n",
    "            else:\n",
    "                self.best_loss = loss_te\n",
    "                self.best_ouputs = te_outputs\n",
    "                self.best_r2_score = r2_res\n",
    "                self.best_model = self.model\n",
    "                self.best_epoch_num = e\n",
    "                print('Trigger Times: 0')\n",
    "                trigger_times = 0  \n",
    "\n",
    "            print(f\"{time_}[{e:02d}/{self.num_epochs}] meta_loss: {meta_loss_sum:.4f} loss_te: {loss_te:.4f}, r2_res = {r2_res:.4f} / trrigger times : {trigger_times}\")  \n",
    "\n",
    "            if breaker == True:\n",
    "                break\n",
    "            ###############################################################\n",
    "\n",
    "\n",
    "        self.name = get_filename('model_save', 'MAML_pretrain', '.pt')\n",
    "        torch.save(self.best_model.state_dict(), os.path.join('model_save', self.name))  # save only state_dict of model\n",
    "        print(f'Saved model state dict! name : {self.name}')\n",
    "        # torch.save(self.best_model, os.path.join('model_save', name))             # save entire model\n",
    "\n",
    "\n",
    "    def validate(self, model):  ###\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_dot_loss = 0        \n",
    "        outputs = torch.Tensor().cuda()\n",
    "        r2_res = 0\n",
    "        with torch.no_grad():\n",
    "            for wk_valid_loader in self.test_dataloaders:\n",
    "                for data, target in wk_valid_loader:\n",
    "                    output = model(data)\n",
    "                    if self.dot:\n",
    "                        d = torch.bmm(model.res_x, model.res_x.transpose(1, 2))\n",
    "                        dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(data.shape[0], 1, 1).cuda())\n",
    "                        loss = (1-self.lamb)*F.mse_loss(output, target) + self.lamb*dot_loss\n",
    "                    else:\n",
    "                        dot_loss = 0\n",
    "                        loss = F.mse_loss(output, target)\n",
    "                    true = target.cpu().detach().numpy().squeeze()\n",
    "                    pred = output.cpu().detach().numpy().squeeze()\n",
    "                    r2_res += r2_score(true, pred)\n",
    "                    total_loss += loss.item()\n",
    "                    total_dot_loss += dot_loss.item()\n",
    "                    outputs = torch.cat((outputs, output))\n",
    "        total_loss /= len(wk_valid_loader) * len(self.test_dataloaders)\n",
    "        total_dot_loss /= len(wk_valid_loader) * len(self.test_dataloaders)\n",
    "        r2_res /= len(wk_valid_loader) * len(self.test_dataloaders)\n",
    "\n",
    "        return total_loss, outputs, r2_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonMAML_pretrain():\n",
    "    def __init__(self, model, train_dataloaders, test_dataloaders, epochs, lr, dot=True, lamb=0.6):\n",
    "    # def __init__(self, model, train_dataloaders, test_dataloaders, meta_tasks, inner_lr, meta_lr, K=K, inner_steps=1):   # K : number of sample data of task\n",
    "       \n",
    "        #############################################\n",
    "        self.patience = 10  # for early stopping \n",
    "        #############################################\n",
    "        \n",
    "        # important objects\n",
    "        self.model = model\n",
    "               \n",
    "        self.train_dataloaders = train_dataloaders\n",
    "        self.test_dataloaders = test_dataloaders\n",
    "        # hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.dot = dot\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.weights = list(self.model.parameters()) \n",
    "        self.optimizer = torch.optim.Adam(self.weights, self.lr)      \n",
    "            \n",
    "        # metrics\n",
    "        self.losses_tr = []\n",
    "        self.losses_te = []\n",
    "        self.r2_score = []\n",
    "        # self.best_loss = 1000\n",
    "        self.best_loss = np.inf\n",
    "    \n",
    "    def train(self):                \n",
    "        trigger_times = 0\n",
    "        breaker = False ####\n",
    "        for e in range(self.epochs):    # epoch     \n",
    "            for data, target in self.train_dataloaders:    \n",
    "                model.zero_grad()             \n",
    "                total_loss_tr = 0\n",
    "                output = model(data)\n",
    "                # loss type\n",
    "                if self.dot:\n",
    "                    d = torch.bmm(model.res_x, model.res_x.transpose(1, 2))\n",
    "                    dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(data.shape[0], 1, 1).cuda())\n",
    "                    batch_loss_tr = (1-self.lamb)*F.mse_loss(output, target) + self.lamb*dot_loss\n",
    "                else:\n",
    "                    dot_loss = 0\n",
    "                    batch_loss_tr = F.mse_loss(output, target)                \n",
    "\n",
    "                batch_loss_tr.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss_tr += batch_loss_tr.item()\n",
    "            loss_tr = total_loss_tr/len(self.train_dataloaders)\n",
    "\n",
    "            loss_te, te_outputs, r2_res = self.validate(model)  ###\n",
    "            self.losses_tr += [loss_tr]\n",
    "            self.losses_te += [loss_te]       \n",
    "            self.r2_score += [r2_res]      \n",
    "            ephch = e + 1\n",
    "            time_ = datetime.datetime.today().strftime('%y%m%d/%H:%M:%S')\n",
    "            # print(f\"{time_}[{ephch:02d}/{self.epochs}] loss_tr: {loss_tr:.8f} loss_te: {loss_te:.8f}, r2_res = {r2_res}\")\n",
    "\n",
    "            if loss_te > self.best_loss:\n",
    "                trigger_times += 1\n",
    "                # print('Trigger Times:' , trigger_times)\n",
    "                if trigger_times >= self.patience:\n",
    "                    print('Early stopping! \\n training step finish')\n",
    "                    breaker = True\n",
    "            else:\n",
    "                self.best_loss = loss_te\n",
    "                self.best_ouputs = te_outputs\n",
    "                self.best_r2_score = r2_res\n",
    "                self.best_model = self.model\n",
    "                self.best_epoch_num = e\n",
    "                print('Trigger Times: 0')\n",
    "                trigger_times = 0  \n",
    "\n",
    "            print(f\"{time_}[{e:02d}/{self.num_epochs}] loss_tr: {loss_tr:.4f} loss_te: {loss_te:.4f}, r2_res = {r2_res:.4f} / trrigger times : {trigger_times}\")  \n",
    "\n",
    "            if breaker == True:\n",
    "                break\n",
    "\n",
    "            # if loss_te < self.best_loss:\n",
    "            #     self.best_loss = loss_te\n",
    "            #     self.best_ouputs = te_outputs\n",
    "            #     self.best_r2_score = r2_res\n",
    "            #     self.best_model = self.model\n",
    "            #     self.best_epoch_num = e\n",
    "        self.name = get_filename('model_save', 'pretrain', '.pt')\n",
    "        torch.save(self.best_model.state_dict(), os.path.join('model_save', self.name))  # save only state_dict of model\n",
    "        print(f'Saved model state dict! name : {self.name}')\n",
    "        # torch.save(self.best_model, os.path.join('model_save', name))             # save entire model\n",
    "\n",
    "        # return self.best_model\n",
    "\n",
    "    def validate(self, model):  ###\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_dot_loss = 0        \n",
    "        outputs = torch.Tensor().cuda()\n",
    "        r2_res = 0\n",
    "        with torch.no_grad():\n",
    "            # for wk_valid_loader in self.test_dataloaders:\n",
    "            # for data, target in wk_valid_loader:\n",
    "            for data, target in self.test_dataloaders:\n",
    "                output = model(data)\n",
    "                if self.dot:\n",
    "                    d = torch.bmm(model.res_x, model.res_x.transpose(1, 2))\n",
    "                    dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(data.shape[0], 1, 1).cuda())\n",
    "                    \n",
    "                    loss = (1-self.lamb)*F.mse_loss(output, target) + self.lamb*dot_loss\n",
    "                else:\n",
    "                    dot_loss = 0\n",
    "                    loss = F.mse_loss(output, target)\n",
    "                true = target.cpu().detach().numpy().squeeze()\n",
    "                pred = output.cpu().detach().numpy().squeeze()\n",
    "                r2_res += r2_score(true, pred)\n",
    "                total_loss += loss.item()\n",
    "                total_dot_loss += dot_loss.item()\n",
    "                outputs = torch.cat((outputs, output))\n",
    "        total_loss /= len(self.test_dataloaders)\n",
    "        total_dot_loss /= len(self.test_dataloaders)\n",
    "        r2_res /= len(self.test_dataloaders)\n",
    "\n",
    "        return total_loss, outputs, r2_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML방식 pretrain\n",
    "model = ReshapeNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, group_dim=group_dim, wk_vec_dim=wk_vec_dim).cuda()\n",
    "maml = MAML_one_batch(model, Train_task_MAML_DL_tr, Train_task_MAML_DL_te, num_epochs=200, inner_lr=0.0005, meta_lr=0.0001)\n",
    "# maml = MAML_one_batch(model, Train_task_MAML_DL_tr, Train_task_MAML_DL_te, num_epochs=200, inner_lr=0.0001, meta_lr=0.0005)   # inner_lr=0.0001, meta_lr=0.0005\n",
    "# maml = MAML_one_batch(model, Train_task_MAML_DL_tr, Train_task_MAML_DL_te, num_epochs=200, inner_lr=0.001, meta_lr=0.0001)    # inner_lr=0.001, meta_lr=0.0001\n",
    "# maml = MAML_one_batch(model, Train_task_MAML_DL_tr, Train_task_MAML_DL_te, num_epochs=200, inner_lr=0.001, meta_lr=0.0005)    # inner_lr=0.001, meta_lr=0.0005\n",
    "\n",
    "maml.main_loop()\n",
    "name_MAML_one_batch_pt = maml.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 학습의 pretrain\n",
    "model = ReshapeNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, group_dim=group_dim, wk_vec_dim=wk_vec_dim).cuda()\n",
    "pretrained_model = nonMAML_pretrain(model, Train_task_pretrain_DL_tr, Train_task_pretrain_DL_te, epochs=200, lr=0.0001)\n",
    "# pretrained_model = nonMAML_pretrain(model, Train_task_pretrain_DL_tr, Train_task_pretrain_DL_te, epochs=200, lr=0.0005)   # lr=0.0005\n",
    "\n",
    "pretrained_model.train()\n",
    "name_nonMAML_pretrain_pt = pretrained_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'-save model- name : {name_MAML_one_batch_pt}')\n",
    "print(f'[MAML training step]    best epoch : {maml.best_epoch_num} / best loss_te : {maml.best_loss:.4f} / best r2 : {maml.best_r2_score:.4f}')\n",
    "print('-----------------------------------------------------------------------------------------')\n",
    "print(f'-save model- name : {name_nonMAML_pretrain_pt}')\n",
    "print(f'[NonMAML training step] best epoch : {pretrained_model.best_epoch_num} / best loss_te : {pretrained_model.best_loss:.4f} / best r2 : {pretrained_model.best_r2_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step result plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_step_result(MAML_loss_tr, MAML_loss_te, MAML_r2, pre_loss_tr, pre_loss_te, pre_r2):\n",
    "#         # 그래프 배열 지정\n",
    "#         f, axes = plt.subplots(2,2)\n",
    "#         f.set_size_inches((12,12))\n",
    "#         plt.subplots_adjust(wspace=0.15, hspace=0.15)   # subplot간의 간격 wspace(좌우), hspace(상하간격)\n",
    "\n",
    "#         # figure 전체 제목\n",
    "#         f.subtitle('Training step result', fontsize = 15)\n",
    "#         # [0,0] 위치 MAML의 train, test loss 비교\n",
    "#         axes[0,0].plot(MAML_loss_tr,label='train_loss')\n",
    "#         axes[0,0].plot(MAML_loss_te,label='test_loss')\n",
    "#         plt.legend()\n",
    "#         plt.title('MAML')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('loss')\n",
    "#         # [0,1] 위치 NonMAML의 train, test loss 비교\n",
    "#         axes[0,1].plot(pre_loss_tr,label='train_loss')\n",
    "#         axes[0,1].plot(pre_loss_te,label='test_loss')\n",
    "#         plt.legend()\n",
    "#         plt.title('pretrain')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('loss')        \n",
    "#         # [1,0] 위치 MAML과 NonMAML의 test loss 비교\n",
    "#         axes[1,0].plot(MAML_loss_te, label='MAML')\n",
    "#         axes[1,0].plot(pre_loss_te, label='pretrain')        \n",
    "#         plt.legend()\n",
    "#         plt.title('test_loss - MAML & pretreain')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('loss')            \n",
    "#         # [1,1] 위치 MAML과 NonMAML의 r2 score 비교\n",
    "#         axes[1,1].plot(MAML_r2, label='MAML')\n",
    "#         axes[1,1].plot(pre_r2, label='pretrain')        \n",
    "#         plt.legend()\n",
    "#         plt.title('R2 score - MAML & pretreain')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('R2')     \n",
    "        \n",
    "#         plt.show()\n",
    "\n",
    "# plot_training_step_result(maml.meta_losses_tr, maml.meta_losses_te, maml.r2_score, \n",
    "#                           pretrained_model.losses_tr, pretrained_model.losses_te, pretrained_model.r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing step and Plotting\n",
    "\n",
    "Now that the model is trained, let's look at how it performs, and compare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_step():\n",
    "    # def __init__(self, trained_model, train_dataloader, test_dataloader, torch_load_filename, epochs=10, repeat_n=5, optim=torch.optim.SGD, dot=True):\n",
    "    def __init__(self, pt_file_path, torch_load_filename, train_dataloader, test_dataloader, epochs=10, repeat_n=5, optim=torch.optim.SGD, dot=True):            \n",
    "        # self.trained_model = trained_model\n",
    "        self.tr_DL = train_dataloader\n",
    "        self.te_DL =test_dataloader\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optim = optim\n",
    "        self.epochs = epochs\n",
    "        # self.batch_num = len(self.tr_DL[0])     # 데이터로더에 있는 배치 개수\n",
    "        self.batch_num = repeat_n     # 평균 구하기 위해 반복하길 원하는 횟수 n\n",
    "        self.wk_num = len(self.tr_DL)            # 데이터셋에 있는 workload 개수\n",
    "        self.dot =dot\n",
    "        self.lamb = 0.6\n",
    "\n",
    "        # result\n",
    "        self.avg_losses_tr = [ [0]*self.epochs ]*self.wk_num\n",
    "        self.avg_losses_te = [ [0]*self.epochs ]*self.wk_num\n",
    "        self.avg_r2_score = [ [0]*self.epochs ]*self.wk_num\n",
    "        self.MODEL_PATH = pt_file_path\n",
    "        self.MODEL_NAME = torch_load_filename\n",
    "\n",
    "    def test_on_workload(self):\n",
    "        model = ReshapeNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, group_dim=group_dim, wk_vec_dim=wk_vec_dim).cuda()\n",
    "        # MODEL_PATH = 'model_save'\n",
    "        # MODEL_NAME = self.torch_load_filename\n",
    "        model.load_state_dict(torch.load(os.path.join(self.MODEL_PATH, self.MODEL_NAME)))\n",
    "        # model.load_state_dict(self.trained_model.state_dict())\n",
    "        optimizer = self.optim(model.parameters(), 0.01)    # need to tuning\n",
    "\n",
    "        losses_tr = []\n",
    "        losses_te = []\n",
    "        r2_score = []\n",
    "\n",
    "        X, y = self.sample_tr[0], self.sample_tr[1]\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(X)\n",
    "            if self.dot:\n",
    "                # d = torch.bmm(self.model.p_res_x, self.model.p_res_x.transpose(1, 2))\n",
    "                d = torch.bmm(model.res_x, model.res_x.transpose(1, 2))\n",
    "                dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(X.shape[0], 1, 1).cuda())\n",
    "                # loss = (1-self.lamb)*self.criterion(model(X), y) + self.lamb*dot_loss\n",
    "                loss = (1-self.lamb)*self.criterion(pred, y) + self.lamb*dot_loss\n",
    "\n",
    "            else:\n",
    "                loss = self.criterion(model(X), y)\n",
    "            losses_tr.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_te, te_ouputs, r2_res = self.validate(model)\n",
    "            # losses_te += loss_te\n",
    "            # r2_score += r2_res\n",
    "            losses_te.append(loss_te) \n",
    "            r2_score.append(r2_res)\n",
    "            # print(loss_te)\n",
    "            # print(losses_te)           \n",
    "\n",
    "            \n",
    "        return losses_tr, losses_te, r2_score         \n",
    "\n",
    "    def average_result(self):\n",
    "        time_ = datetime.datetime.today().strftime('%y%m%d/%H:%M:%S')\n",
    "        print(f\"[{time_}] train start\")\n",
    "        for wk in range(self.wk_num):\n",
    "            # sampler_tr = Sampler(dataloaders=self.tr_DL[wk])   \n",
    "            sampler_tr = iter(self.tr_DL[wk])            \n",
    "         \n",
    "            for n_batch in range(self.batch_num):   # 데이터로더의 배치 수 만큼           \n",
    "                  # len(self.tr_DL): workload개수\n",
    "                sample_tr = next(sampler_tr)        # wk번째 데이터 로드의 다음 배치데이터           \n",
    "                self.sample_tr = sample_tr    \n",
    "                self.te_DL_wk = self.te_DL[wk]\n",
    "\n",
    "                losses_tr, losses_te, r2_core = self.test_on_workload()\n",
    "                self.avg_losses_tr[wk] = [l + l_new for l, l_new in zip(self.avg_losses_tr[wk], losses_tr)]\n",
    "                self.avg_losses_te[wk] = [l + l_new for l, l_new in zip(self.avg_losses_te[wk], losses_te)]\n",
    "                self.avg_r2_score[wk] = [r + r_new for r, r_new in zip(self.avg_r2_score[wk], r2_core)]\n",
    "                \n",
    "                #################################################################################\n",
    "                self.temp_avg_losses_tr[wk] = [l / n_batch for l in self.avg_losses_tr[wk]]\n",
    "                self.temp_avg_losses_te[wk] = [l / n_batch for l in self.avg_losses_te[wk]]\n",
    "                self.temp_avg_r2_score[wk] = [l / n_batch for l in self.avg_r2_score[wk]]\n",
    "                #################################################################################\n",
    "                # print('self.avg_losses_tr : -------------------------')\n",
    "                # print(self.avg_r2_score)\n",
    "                # print('self.avg_losses_tr[wk] : -------------------------')\n",
    "                # print(self.avg_r2_score[wk])\n",
    "            time_ = datetime.datetime.today().strftime('%y%m%d/%H:%M:%S')\n",
    "            print(f'[{time_}] workload {wk+1}번째 끝')\n",
    "            self.avg_losses_tr[wk] = [l / self.batch_num for l in self.avg_losses_tr[wk]]\n",
    "            self.avg_losses_te[wk] = [l / self.batch_num for l in self.avg_losses_te[wk]]\n",
    "            self.avg_r2_score[wk] = [l / self.batch_num for l in self.avg_r2_score[wk]]\n",
    "\n",
    "            # print('평균 낸 후 self.avg_losses_tr[wk] : -------------------------')\n",
    "            # # print(self.avg_r2_score[wk])\n",
    "                       \n",
    "\n",
    "        return self.avg_losses_tr, self.avg_losses_te, self.avg_r2_score\n",
    "\n",
    "        # for n_batch in range(self.batch_num):  # 데이터로더의 배치 수 만큼\n",
    "        #     sample_tr = sampler_tr.get_sample()\n",
    "        #     for wk in range(self.wk_num):      # len(self.tr_DL): workload개수\n",
    "        #         self.sample_tr = sample_tr[wk]    \n",
    "        #         self.te_DL_wk = self.te_DL[wk]            \n",
    "        #         losses_tr, losses_te, r2_core = self.test_on_workload()\n",
    "        #         avg_losses_tr = [l + l_new for l, l_new in zip(avg_losses_tr, losses_tr)]\n",
    "        #         avg_losses_te = [l + l_new for l, l_new in zip(avg_losses_te, losses_te)]\n",
    "        #         avg_r2_score = [r + r_new for r, r_new in zip(avg_r2_score, r2_core)]\n",
    "        # avg_losses_tr = [l / self.batch_num for l in avg_losses_tr]\n",
    "        # avg_losses_te = [l / self.batch_num for l in avg_losses_te]    \n",
    "        # avg_r2_score = [l / self.batch_num for l in avg_r2_score]        \n",
    "\n",
    "        # return avg_losses_tr, avg_losses_te, avg_r2_score\n",
    "\n",
    "    def validate(self, model):  ###\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_dot_loss = 0        \n",
    "        outputs = torch.Tensor().cuda()\n",
    "        r2_res = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.te_DL_wk:\n",
    "                output = model(data)\n",
    "                if self.dot:\n",
    "                    d = torch.bmm(model.res_x, model.res_x.transpose(1, 2))\n",
    "                    dot_loss = F.mse_loss(d, torch.eye(d.size(1)).repeat(data.shape[0], 1, 1).cuda())\n",
    "                    loss = (1-self.lamb)*F.mse_loss(output, target) + self.lamb*dot_loss\n",
    "                else:\n",
    "                    dot_loss = 0\n",
    "                    loss = F.mse_loss(output, target)\n",
    "                true = target.cpu().detach().numpy().squeeze()\n",
    "                pred = output.cpu().detach().numpy().squeeze()\n",
    "                r2_res += r2_score(true, pred)\n",
    "                total_loss += loss.item()\n",
    "                total_dot_loss += dot_loss.item()\n",
    "                outputs = torch.cat((outputs, output))\n",
    "        total_loss /= len(self.te_DL_wk) #* len(self.te_DL)\n",
    "        total_dot_loss /= len(self.te_DL_wk) #* len(self.te_DL)\n",
    "        r2_res /= len(self.te_DL_wk) #* len(self.te_DL)\n",
    "\n",
    "        return total_loss, outputs, r2_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make tesing step dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preparing for tesing step dataset'''\n",
    "\n",
    "wk_type = {0:'WR 9:1 (1)',  1:'WR 9:1 (4)',  2:'WR 9:1 (16)',  3:'WR 9:1 (64)',\n",
    "           4:'WR 1:1 (1)',  5:'WR 1:1 (4)',  6:'WR 1:1 (16)',  7:'WR 1:1 (64)',\n",
    "           8:'WR 1:9 (1)',  9:'WR 1:9 (4)',  10:'WR 1:9 (16)', 11:'WR 1:9 (64)',\n",
    "           12:'UPdate (1)', 13:'UPdate (4)', 14:'UPdate (16)', 15:'UPdate (64)'}\n",
    "\n",
    "K_te = 10   # batch size of testing step\n",
    "wk_using_testing_step = [2,6,10,14]\n",
    "\n",
    "def make_wk_type_using(wk_using_testing_step):    \n",
    "    wk_type_using = []\n",
    "    for i in range(len(wk_using_testing_step)):    \n",
    "        wk_type_using.append(wk_type[wk_using_testing_step[i]])\n",
    "\n",
    "wk_type_using = make_wk_type_using(wk_using_testing_step)\n",
    "\n",
    "# # for dataloader new task test(MAML, pretrain)\n",
    "New_task_DL_tr, New_task_DL_te, New_task_X_te, New_task_y_te = MAML_dataset(entire_X_tr, entire_y_tr, entire_X_te, entire_y_te, scaler_X, scaler_y, wk_using_testing_step, batch_size=K_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r: 빨강\n",
    "g: 초록\n",
    "b: 파랑\n",
    "black : 검정\n",
    "c: 시안\n",
    "m: 마젠타"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wk_vec = [[9,1,0,1], [9,1,0,4], [9,1,0,16], [9,1,0,64],\n",
    "            [1,1,0,1], [1,1,0,4], [1,1,0,16], [1,1,0,64],\n",
    "            [1,9,0,1], [1,9,0,4], [1,9,0,16], [1,9,0,64],\n",
    "            [0,0,1,1], [0,0,1,4], [0,0,1,16], [0,0,1,64]]\n",
    "\n",
    "wk_using_training_step = [0,1,3,\n",
    "                          4,5,7,\n",
    "                          8,9,11,\n",
    "                          12,13,15]\n",
    "\n",
    "wk_using_test_step = [2,6,10,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAML_avg_losses_tr, MAML_avg_losses_te, MAML_avg_r2_score = test_step('model_save',name_MAML_one_batch_pt, New_task_DL_tr, New_task_DL_te, epochs=10, repeat_n=100).average_result()\n",
    "MAML_avg_losses_tr, MAML_avg_losses_te, MAML_avg_r2_score = test_step('model_save', '20220425/MAML_pretrain-20220424-02.p', New_task_DL_tr, New_task_DL_te, epochs=10, repeat_n=100).average_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_avg_losses_tr, pre_avg_losses_te, pre_avg_r2_score = test_step('model_save',name_nonMAML_pretrain_pt, New_task_DL_tr, New_task_DL_te, epochs=10, repeat_n=100).average_result()\n",
    "pre_avg_losses_tr, pre_avg_losses_te, pre_avg_r2_score = test_step('model_save','20220425/pretrain-20220424-03.pt', New_task_DL_tr, New_task_DL_te, epochs=10, repeat_n=100).average_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing step result plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maml.meta_losses_tr\n",
    "maml.meta_losses_te\n",
    "maml.r2_score\n",
    "        \n",
    "pretrained_model.losses_tr\n",
    "pretrained_model.losses_te\n",
    "pretrained_model.r2_score\n",
    "\n",
    "MAML_avg_losses_tr\n",
    "MAML_avg_losses_te\n",
    "MAML_avg_r2_score\n",
    "\n",
    "pre_avg_losses_tr\n",
    "pre_avg_losses_te\n",
    "pre_avg_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAML_avg_losses_te[0], label='WR9:1 (16)')\n",
    "plt.plot(MAML_avg_losses_te[1], label='WR1:1 (16)')\n",
    "plt.plot(MAML_avg_losses_te[2], label='WR1:9 (16)')\n",
    "plt.plot(MAML_avg_losses_te[3], label='Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss_te')\n",
    "plt.title('Testing step_MAML (loss_te)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_avg_losses_te[0], label='WR9:1 (16)')\n",
    "plt.plot(pre_avg_losses_te[1], label='WR1:1 (16)')\n",
    "plt.plot(pre_avg_losses_te[2], label='WR1:9 (16)')\n",
    "plt.plot(pre_avg_losses_te[3], label='Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss_te')\n",
    "plt.title('Testing step_pre (loss_te)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MAML_avg_r2_score[0], label='WR9:1 (16)')\n",
    "plt.plot(MAML_avg_r2_score[1], label='WR1:1 (16)')\n",
    "plt.plot(MAML_avg_r2_score[2], label='WR1:9 (16)')\n",
    "plt.plot(MAML_avg_r2_score[3], label='Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('r2-score')\n",
    "plt.title('Testing step_MAML (r2) ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_avg_r2_score[0], label='WR9:1 (16)')\n",
    "plt.plot(pre_avg_r2_score[1], label='WR1:1 (16)')\n",
    "plt.plot(pre_avg_r2_score[2], label='WR1:9 (16)')\n",
    "plt.plot(pre_avg_r2_score[3], label='Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('r2-score')\n",
    "plt.title('Testing step_pre (r2)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_avg_losses_te[0], label='P_WR9:1 (16)')\n",
    "plt.plot(pre_avg_losses_te[1], label='P_WR1:1 (16)')\n",
    "plt.plot(pre_avg_losses_te[2], label='P_WR1:9 (16)')\n",
    "plt.plot(pre_avg_losses_te[3], label='P_Update(16)')\n",
    "\n",
    "plt.plot(MAML_avg_losses_te[0], label='M_WR9:1 (16)')\n",
    "plt.plot(MAML_avg_losses_te[1], label='M_WR1:1 (16)')\n",
    "plt.plot(MAML_avg_losses_te[2], label='M_WR1:9 (16)')\n",
    "plt.plot(MAML_avg_losses_te[3], label='M_Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss_te')\n",
    "plt.title('Testing step_MAML & pre (loss_te) ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pre_avg_r2_score[0], label='P_WR9:1 (16)')\n",
    "plt.plot(pre_avg_r2_score[1], label='P_WR1:1 (16)')\n",
    "plt.plot(pre_avg_r2_score[2], label='P_WR1:9 (16)')\n",
    "plt.plot(pre_avg_r2_score[3], label='P_Update(16)')\n",
    "\n",
    "plt.plot(MAML_avg_r2_score[0], label='M_WR9:1 (16)')\n",
    "plt.plot(MAML_avg_r2_score[1], label='M_WR1:1 (16)')\n",
    "plt.plot(MAML_avg_r2_score[2], label='M_WR1:9 (16)')\n",
    "plt.plot(MAML_avg_r2_score[3], label='M_Update(16)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('r2-score')\n",
    "plt.title('Testing step_MAML & pre (r2) ')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_step_result(wk_type_using, MAML_avg_losses_tr, MAML_avg_losses_te, MAML_avg_r2_score, pre_avg_losses_tr, pre_avg_losses_te, pre_avg_r2_score, wk_type_using):\n",
    "#         wk_num = len(MAML_avg_losses_tr)\n",
    "#         # 그래프 배열 지정\n",
    "#         f, axes = plt.subplots(3,2)\n",
    "#         f.set_size_inches((12,12))\n",
    "#         plt.subplots_adjust(wspace=0.15, hspace=0.15)   # subplot간의 간격 wspace(좌우), hspace(상하간격)\n",
    "\n",
    "#         # figure 전체 제목\n",
    "#         f.subtitle('Testing step result', fontsize = 15)\n",
    "#         # [0,0] 위치 MAML의 testing step에서의 test loss 비교\n",
    "#         for i in range(wk_num):\n",
    "#                 axes[0,0].plot(MAML_avg_losses_te[i],label=f'{wk_type_using[i]}')\n",
    "#         axes[0,0].set_tile('MAML loss_te')\n",
    "#         plt.legend()\n",
    "#         # plt.title('MAML')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('loss')\n",
    "#         # [0,1] 위치 NonMAML의 testing step에서의 test loss 비교\n",
    "#         for i in range(wk_num):\n",
    "#                 axes[0,0].plot(pre_avg_losses_te[i],label=f'{wk_type_using[i]}')\n",
    "#         axes[0,0].set_tile('pre loss_te')\n",
    "#         plt.legend()\n",
    "#         # plt.title('MAML')\n",
    "#         plt.xlabel('epochs')\n",
    "#         plt.ylabel('loss')\n",
    "\n",
    "# #       plt.xlabel('epochs')가 아니라 axes[?,?].set_xlabel()을 해야하나?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         plt.show()\n",
    "\n",
    "# plot_training_step_result(maml.meta_losses_tr, maml.meta_losses_te, maml.r2_score, \n",
    "#                           pretrained_model.losses_tr, pretrained_model.losses_te, pretrained_model.r2_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
